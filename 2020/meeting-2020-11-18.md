Shall we hold a meeting today at 16:00 UTC or not?

I'd say yes, it might be a short one but we should keep all meetings in the case someone want to join.

yeah, let's meet

link?

[Click to join video call](https://meet.jit.si/883733490973467)

oh, its a call, thought it was text

ah - we can do text instead - good with either route

let's do it by text

So!

Hello everyone

Hi!

hi

so last week we slowly started working on the project

yep, we started the RFCs

we decided to put the RFCs code on hackmd to accelerate the RFC development process

without clashing with each other

but that is being mirrored on the official github

at farcaster-project

what are the steps to use the hackmd?

I added the current team members

oh, forgot Kayaba

so everyone could edit, but it only makes sense to use for the people that are actively working on it

especially if working at the same time

anyhow, the normal github style works as well

for each new file, we have to link it though to a new hackmd file

@**h4sh3d** any specific questions?

I think it's all good. Just use, on GitHub, the "Collaborate on HackMD" button and connect with your github account to edit.

ping me if u have issues

thanks @**zkao** for this setup

Also, it's possible to switch branches by changing the branch that one pulls from, but that switches the active branch for everyone.

i bridged the entire #general stream in zulip with freenode #monero-swap, so whatever we discuss here will end up in freenode.

@**lederstrumpf** we created a hackmd branch, USE THAT ONE ONLY

ack

Ok. I propose we move on RFCs. I can describe the goal and the current state

yes

The goal of these RFCs is to discuss the high level structure and the general design of farcaster.

Currently we have one main RFC called "architecture". In this RFC we started describing the different component and we started putting names on their interactions.

The current architecture is the same as presented in the CCS proposal and nothing tells us to change it for now.

my take is that we should discuss more on the terminology, because i was finding very hard to understand many things, especially the proposed UX centric approach of the external API design

or rather, relate to that approach, but the names were more daemon-centric, than client-centric

I think it's because the component that interact with all other components is the daemon

And the swap, despite the fact that it's piloted by a client, is executed inside the daemon

But yes, names can change

@**lederstrumpf**  is the one that had strong sentiments on the UX-centric design

it's also true that I reason with a daemon-centric approach

sure, but that does not dictate naming

I think there is something fucked with regards to zulip bridge

LGTM, why?

@**TheCharlatan** wrote there, and it never made it in zulip

Oh yes, I see

it used to work

will look into that later

anyway: being UX centric does not dictate naming

ok. so we agree that the name can be better than now, but does not need to be ux centric

ack

ack

On thing that has been discuss here in the past days is the recovery mechanism

We have started another RFC about the "jobs", the stuff sent by the daemon to a syncer to execute some specific work

What we describe in the job and architecture RFCs is that the outputs of the daemon MUST be replayable without side effect, thus allowing a recovery process for a previous saved state.

Maybe I can talk quickly about daemon outputs

in practice that is crazy hard to achieve, but, yeah, i like to code this kind of stuff

^ yes :)

I've tried to think about the "state transition" for the swap

I've started something here: https://github.com/h4sh3d/RFCs/blob/swap-state/swap-state.mediawiki

My idea is the following: the daemon will receive inputs, from different sources.

Client, syncers, counter-party daemon, and maybe other (e.g. from itself)

We have a start state, and a transition is applying one input on that state.

so a transition is the tuple `(state, input)`, and there is some specific computation that must be done for that specific tuple

e.g. `(state: start, input: client_key_initialized)`

inital state:
$$() \rightarrow State_0$$


state update:
$$(State, Input) \rightarrow State$$

that is how it generaly works

Yep. And a state can take multiple inputs, but only one at once. So order matters

Example

If state `A` can take inputs `I1` or `I2`

and `(A, I1) -> A1` and `(A,I2) -> A2`

but `A1` cannot take `I2` as input, if you take one path you cannot go back

well, sometimes it will be valid

Yep, that is handled by liner types in rust

that's gonna be a design decision

So, if we can represent all the state and all there possible input, we can create the recipe for the swap.

Sorry, but this is why I want to use petrinets for the runtime, so that we dont have to mess up with these things

some transitions will commute, others won't

In some cases we want `((A, I1), I2)` to be possible, in other we dont

Yes, petrinet is a good tool for that

we can encode that in a petrinet

ok, seems like we agree ^^

Now, each of these transition will produce outputs

when u say outputs, do u mean side effects, by any chance?

those outputs will be directed to syncers, client, and daemon

ok, that means side effects

by output I mean: jobs, action request (in the current RFC arch diagram), etc.

so yes, those will produce side effects

BUT

we should make sure that all those outputs can be replayed safely

And then we have a recovery mechanism

Im not totally sure about the playing safely meaning -- is it because we lost state that we have to replay safely?

Because you can restart from a previous state and apply daemon inputs, producing daemon outputs, that reproduce the following daemon inputs, etc.

Yes, exactly. For example, if we saved on disk state `Ax`, and we started applying `Ix` on it, the computation starts and produce a job for the btc syncer: broadcast tx `Z`, and then it crashes before the new save on disk

When we restart the daemon with state `Ax` we will redo the broadcast job `Z`, and it must not be a problem

This is the basic idea about driving the swap, within a model that allow us to describe all the action the syncers, client, daemon must do, and how to recover from a crash. But this is an idea

So i guess i get it: daemon submits a job to syncer, and is waiting for a result from the syncer, but daemon crashes before syncer publishes tx to blockchain and acknowledges back

yes

but for me the state is AWAITING_CONFIRMATION

And there is still some missing parts:
You saved state `Ax`, you receive `Ix` and you crash, you restart with `Ax`. But to continue you should receives `Ix`... but, e.g., syncer already send it, how do you get it again?

Either you saved it at reception, or at each state you have a series of default outputs to do

I prefer the second one

the state of what is AWAITING

im just afraid youre trying to reinvent very well established methods of doing this

all these wrapped structures like result futures options, they all do these kind of thing

I think I'm explaining something I do have the right name already

and the way you discretize state is not obvious

No, I think it's at a different level, here I want to represent what are the actions possible from a protocol point of view. And we should not think about it as a linear thing

guys, it s 17:00, we should wrap up

Next actions:
discussing more about the names
think about how we should represent the swap protocol, in a way we can code

what else?

we bought farcaster.dev domain name, possibly we could use community help to do something there

community help wouldnt hurt for a logo as well

a website pointing to the github, the zulip and other resources would be great

+1 for a logo

we keep async communicating on the architecture topic, cheers

cheers

ciao, thanks for the meeting

Sorry for missing this. It completely slipped my mind. Made sure my calendar alert for Wednesdays is active.

@**zkao** Yep, was wondering about access to those docs. I am on HackMD as https://hackmd.io/@5SK1KnYATKeh18LdJ4mN3A. Would appreciate an invite.

added, sorry about that

**kayabaNerve_**: All good. Thank you