we'll do  a meeting at 16:00 UTC

I'll be here! Jitsi or text?

text

:wave:

:wave:

**zkao**: hello

so @**h4sh3d** will be a few minutes late

but we can get started with what we've been doing in the past week

following our discussion on orchestrating the prtocol, we improved some ideas about protocol execution

hi!

I worked offline for now on the use case RFC, defining a bit better what the UI can look like.

I also worked a bit on the jobs RFC: https://github.com/farcaster-project/RFCs/blob/hackmd/jobs.md

nice, that is new, have to read it

this describe the messages (API) exchanged between the daemon and the syncers

I'm happy to get feedback on that one in the following days

For the use case I plan to have it published on Friday

I've been keeping an eye on things, yet have yet to directly contribute. I did have a few questions about the way the RFCs are going though, mainly:

- Why would the RFC need to define a loopback message? I fully understand why a daemon would keep a queue of active jobs, as necessary. My question is why we define it as a message in the RFC itself. Shouldn't it be an implementation detail?

- What do you all think about standardizing jobs? Obviously, watching a BTC TX is different from a XMR TX, I know. The existing job definition notes the arguments each coin needs. That said, if we want to try to create a more cohesive ecosystem, the RFCs themselves can define a `watch` job (with each coin defining arguments), just as we already have a `publish` job.

For the first question: I defined it as a message because it's part of the swap state.

*So yes, I am basing my latter question off the that markdown. I actually really support it so far. My Q is should every type of job be defined by the core RFCs, in order to not risk a XMPP situation where everyone writes their own extensions leading to fragmentation. Even with manually defining jobs, there can be argument fragmentation on a per-coin level, but I don't see that as too much of a problem.

I started explaining my thoughts on that in the State chapter of the Architectue RFC: https://github.com/farcaster-project/RFCs/blob/hackmd/architecture.md#swap-state

i do agree the loopback is an implementation detail and it probably isnt a message, its just function calls btw different parts of the daemon

Thanks for clarifying, h4sh3d :)

loopback is a wierd one yes, it's not part of the daemon's API, as it's internal, but it matters for what needs to be saved on disk in case of recovery. Is it a message, I don't know. What I know is that it's part of the state as the other daemon input messages

And sorry for starting the meeting with the large questions. Just wanted to make sure I asked them. Happy to wait for longer discussions on the second question after everyone comments what they've been up to, which I think is how @**zkao** wanted to start this.

For your second Q kayaba: it's a good question. I didn't think about that. If it makes more sense to have one watch then we change.

ok :)
@**zkao** do you want to continue on questions or move on and discuss that latter?

nope

I do think we have to discuss a lot about trying to standarlize stuff

kayaba, I also don't think framing the intra-daemon communication as "messages" is necessary. As h4sh3d mentioned, if we want to replay the state in a recovery purely from messages, then these need to be included as "messages". If it turns out that we can't do this, I'd ultimately drop loopback messages from the spec.

because this project is about interoperability

@**lederstrumpf** If the daemon is reloading a state, I don't know why that wouldn't be completely internal. I agree we need to standardize required features, and external routes, yet the daemon reloading a state from disk shouldn't be required to be based on messages IMO; just be able to output the relevant messages if needed by something it's communicating with.

And then if a client is recreating a state with a different daemon instance, it doesn't have access to the loopback messages.

we're on the exploration phase, and we r trying to create a language to understand each other currently. i guess i agree with you, but i know everything  will change anyway! haha

I see, for me that part was not exclusively about interoperability, but also to reason about the swap and help architecturing the code latter

sorry, i meant interoperability in regards to topic 2 of @**Kayaba**'s initial message

namely

> What do you all think about standardizing jobs? Obviously, watching a BTC TX is different from a XMR TX, I know. The existing job definition notes the arguments each coin needs. That said, if we want to try to create a more cohesive ecosystem, the RFCs themselves can define a watch job (with each coin defining arguments), just as we already have a publish job.

> And then if a client is recreating a state with a different daemon instance, it doesn't have access to the loopback messages.

This is true, but neither will it have access to the inter-daemon messages nor the syncer<->daemon messages: the client only has access to client<->daemon communication.

I really think it comes down to how interoperable do we want to be. We of course want the entire ecosystem to work together, but what about two different XMR daemon implementations? How can we require they do the exact same arguments by the core spec without also convoluting BTC (or hard specifying parameters per coin)? And then even if we define arguments on a per-coin basis, do we want to be able to directly move over save files as well?

I personally would say we don't want to require two different XMR daemons have the same API (referring to the watch TX job's arguments, though there's no reason they wouldn't simply for convenience; both the private view/public spend...), just that they have the same job types and overall functionality.

I also think wanting to support saved job states on the file level is an extremely impractical task, which I brought up to make a point.

only the daemon has access to all messages (on one side of the swap) since it's the centrepiece for all communication.

Yep. I was more thinking of daemon reconnecting. If a daemon crashes, and you need to recover your swap (hopefully successfully since daemons support disconnects/reconnects with the other party), it may make sense for clients to implement a way to just quickly publish its own copy of the swap state messages to rebuild it on the daemon side. That said, proper recovery would be the daemon rebooting, and it should never crash in the first place. I was trying to note how loopback messages are in this weird middle ground.

Anyways. Been talking a lot. Definitely going to tone it down, sorry about that. Last thing I wanted to say is @**h4sh3d** noted using the RFCs not only for the protocol yet also for designing code (which is also something I appreciate :) ). Would it make sense to explicitly separate the two/define recommended implementation practices? MUST/MUST NOT/SHOULD...

i think you're raising very relevant questions, @**Kayaba**

I think it makes sense to separate what is for interoperability and what is farcaster project

yes very relevant, thanks

@**Kayaba** what do you think about the general segregation of the 3 components?

Client Daemon Syncer

it was based on requirements:

client has access to secret keys

remote syncers can be used by light wallets

etc

I think it makes a lot of sense. You have an SPV client which solely watches the chain, a daemon managing the logic and communicating in its own way (to execute the swap; signature transmission and all that), and then the actual wallet. Definitely something I like

light-wallet -- no full node

i dislike the amount of loops the current RFCs have, because it gets very overwhelming to analyse

* the current interaction of the components have

Of course, not just a SPV client. BTCd would also work. Just noting on a practical level, people will use light clients. I am curious how it'll end up when they're deployed as libraries instead of microservices (as the docs kinda assume they are by the communication ideology).

my take is library first priority, microservice second

That said, while a single executable breaks down the security of key separation, I do really appreciate the design allows it if deployed as microservices :) More a thought then something I want to propose working on.

Yep. I will note that daemon <-> syncer has a good chance though. A lot of parties may rely on Electrum instead of shipping their own SPV impl.

and a lot of these loopback on the daemon (my understanding) is just two daemon libs talking to each other

@_**zkao|361558** [said](https://farcaster.zulipchat.com/#narrow/stream/263665-general/topic/meetings/near/217907724):
```quote
i dislike the amount of loops the current RFCs have, because it gets very overwhelming to analyse
```
Do you mean with the diagram or...?

yes, we can wrap everything into one binary, two (client+daemon/syncer), three... it a good option to have

*Where of course, the diagram is representative of the protocol

especially for deving

the important point for me: the message passing interaction btw components suggests an event driven architecture

even if we initally build a big binary, internally it should still be an event driven system

totally asynchronous

and for that Rust is not a bad choice!

that im not sure

we talked a bit about framework like actix, I see a good fit for that

i read a bit about it, and trully dislike it

While I agree about making it event driven, it is important to think about how any single swap will likely want to sequentially execute jobs (just so it can purely use await and not track futures as futures). Could be wrong in this thinking, please feel free to call me out :P

Isn't tokio where the majority is?

im not sure we're yet ready to decide what specific libraries to use for these, but i think tokio will be at the heart of any of these async libs

for async await, the executor is tokio mainly yes.

reactive functional programing is a better model than actor-based, as far as correctness is concerned

It may be helpful to look over asmr? It's obviously no where near Farcaster but it is a Rust impl taking generics to heart

@**Kayaba** the jobs will probably be pretty sequential, but you can have client interaction (start cancelling the swap), counter-party daemon, self-daemon (timer), syncers (blockchain events) that makes it less sequential

@**h4sh3d** When you're only dealing with a single swap at a time, the only non-sequential event should be an end-user driven cancel AFAICT. It'll look for a TX, find it, ask for the latest swap info from the counterparty, succeed/fail, move on...

Yes asmr is very good for being blockchain agnostic, and I really like the names for "Alice" and "Bob" roles with Unscripted and Scripted chains

@**Kayaba** Im trying to orchestrate the protocol as a petrinet, i can show you some stuff later

its a great mathematical description for concurrency

Because yes, the daemon as a whole will need to be very event driven. I'm referring to a singular swap to think of making a clean implementation ;) A design which is meant with ease of development in mind...

@**Kayaba** yes, but it can come from your client or from the counterparty detected on-chain

@**Kayaba** implement Syncer<B, A> Syncer<A,B> trait, Client<T> trait and Daemon<T> trait and u r done

I considered counterparty cancellation as a failure leading to recovery :P The Future still resolves, without being cancelled/replaced/running at the same time as another future in the same swap. It just resolves as an error.

and the rest can be forced to be sequential or not (who sends the first messages in the initialization phase: Alice or Bob?)

@_**zkao|361558** [said](https://farcaster.zulipchat.com/#narrow/stream/263665-general/topic/meetings/near/217909389):
```quote
@**Kayaba** implement Syncer<B, A> Syncer<A,B> trait, Client<T> trait and Daemon<T> trait and u r done
```
Sounds complicated; can we just do `unsafe {let mut please: AtomicSwap<BTC, XMR>;}` and call it a day?

Ok, that's a way to deal with it

hahaha

@**Kayaba** please go ahead do it

i make sure sure it gets merged

haha

if u can derive all the rest from preimplemented traits

Rust `unsafe` resolve everything :)

Considering we already have to handle cancellation from our user, we'll still have future management @**h4sh3d**. We can see when the time comes :) Appreciate the discussion now though

Yes me too. I'm currently trying to not link my reflexion on any specific way of implementing it (it has pros and cons)

So, what's next

@**Kayaba** do u really think its that hard to generalize a syncer's trait?

havent u done something like that on asmr?

It was 100% a joke :P asmr is fully generic

ok

I do have one other question

@**Kayaba** could u please review @**h4sh3d** work on the jobs?

RFC

Sure. Was also planning on writing up what making everything `common` would look like, just so we can see the option in front of us.

what is ur question @**Kayaba** ?

Never mind on my other question. It breaks the sandbox too much. I was thinking if we could do:
Syncer<T>
Daemon<A, B>
Client<A> instead of Client<A, B>

you thought of these better than anyone else, im sure :D

As in, creating a single wallet for each coin and using that as a client, solely carrying a destination address along the way. It gives the daemon control though because it'd need to provide the TX which needs to be signed

> Client<A> instead of Client<A, B>

Depends on how dumb the client will be

Which allows arbitrary signing, yet we already assume the daemon is trusted?

or rather: how little they can safely know

for now, we assume the daemon is fully trusted

Yeah. It would be nicer if we could do XmrWallet instead of Client(XmrWallet, BtcWallet). That said, it does move the daemon to verifying the B side's data AND providing the final TX for the private key to sign to claim the funds.

So you'd have a Client<Bitcoin> that can do bitcoin monero and bitcoin litecoin swaps?

yes

It really just splits the client into client/daemon. Doesn't work out.

Right, with the Daemon being instantiated with both and picking up the pieces. That's why it doesn't work out.

It's still somewhat interesting? May provide some good idea on implementation? But I think it puts too much trust on the daemon

I think we can have generics, and specific implementations of generics that fix one of the type parameters

say Client<Bitcoin,T>

Because it is already trustless, but it does say, in the end, the daemon provides arbitrary data to be signed, and in that case, we may as well give it the private key.

Agreed.

I was going to ask about it before I thought it over a bit more :P Explained it here more to note it than anything else.

the daemon is fully trusted at the moment, so dont feel shy to feed in new ideas

Thank you :)

to a good extend, this is a trial and error process, but please no errors hahaha

Agree that in that case the private keys can be moved into the daemon (if that's what you're saying), the segregation is to allow more types of client, and maybe some where you don't have access to the keys

but both are fully trusted piece of software from the user perspective

and I think it's reasonable, but feel free to comment on it

hmm, letting the daemon know about private keys directly will create future problems

Right, in this theoretical, we may as well move private keys to the daemon. That's why I said it doesn't work out, and I solely thought it was interesting and something I wanted to note, nothing more.

(and the daemon communicates with the counterparty, meaning the principal attack target)

@_**zkao|361558** [said](https://farcaster.zulipchat.com/#narrow/stream/263665-general/topic/meetings/near/217911661):
```quote
hmm, letting the daemon know about private keys directly will create future problems
```
Hardware wallets being the biggest headache I can think of

shall we wrap up, now should be the MRL meeting

at #monero-research-lab

ye

yes, thanks everyone. feel free to come here chilling around :)

Very happy to have been here! Really happy with where the project is going in the future

cheers!